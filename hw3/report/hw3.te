% First we choose the document class (can be book, report, article etc.)
\documentclass[11pt]{article}
\usepackage{amsmath}

\usepackage{xcolor}
\usepackage{listings}

\lstdefinestyle{customc}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  frame=L,
  xleftmargin=\parindent,
  language=C,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\bfseries\color{green!40!black},
  commentstyle=\ttfamily\color{purple!40!black},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
}
\lstset{escapechar=@,style=customc}

\author{Edward LaFemina \\
		\it{University of Maryland, Baltimore County}}
\title{Math 627 HW 2}
\date{\today}

\begin{document}
\maketitle
\tableofcontents

\pagebreak
\section{Problem 1}
I will be completing exercises 1, 2, and 3 in Chapter 3 of Pacheco.
	\subsection{Exercise 1}
	This goal of this exercise is to run the given \texttt{greetings.c} program written by Pacheco first on one processor, then on as many as possible and compare the outputs.
	
	I first downloaded the greetings.c source code from the Pacheco website and copied and pasted it onto the maya cluster in my math627 directory. To compile, I used the command \texttt{mpiicc greetings.c -o greetings}. To run my program on only one process, I created a file called \texttt{run\_greetings\_1\_1.slurm} which contains directives used by slurm to run my code. I set the job-name to greetings, the partition to develop, nodes to $1$, ntasks-per-node to $1$, and, adopting a naming convention of \textless job-name\textgreater\_\textless nodes\textgreater\_\textless processes per node\textgreater\_slurm, I named my output and error files with file extensions .out and .err respectively. I then ran my code using the command \texttt{sbatch run\_greetings\_1\_1.slurm}.
	
	My code ran without error and the output and error files were both empty. I believe this to be correct because the code copied from Pacheco only listens for messages on process 0 without creating any of its own. Thus, no messages are sent and none can be received so the output remains blank. Keeping on the development partition to avoid interfering with research projects, a table on the maya resource page states there are 6 nodes I can run on and a paragraph of text above the table says I can only use $5$ nodes, however I found that though I can only use $5$ nodes, the ones I can use are slightly different than those listed in the paragraph. I can run my code on $5$ nodes, $2$ hpcf2013, $1$ hpcf2010, and $2$ hpcf2009 nodes, for a total of $48$ processes. Because slurm limits the ntasks-per-node value to the minimum of the number or processes the requested nodes can handle, I will only run my code on $5$ nodes with $8$ processes each for a total of $40$ processes. To do this, I will make a new \texttt{.slurm} file following my naming convention and setting nodes to $5$ and ntasks-per-node to $8$.
	
	When this completes, my .out file is filled with the greeting from processes $1$ through $39$ inclusive and in order, least to greatest. This is expected because of the way process $0$ is listening for messages. It listens first for messages from process $1$, then process $2$, etc up to the last process, process $39$.
	\pagebreak

	\lstinputlisting[language=C, style=customc, caption=The C source code I used\, taken straight from the Pacheco website:]{../snapshots/greetings_1.c}
	
	\lstinputlisting[language=bash, caption=\texttt{run\_greetings\_1\_1.slurm} used for running on a single process:]{../run_greetings_1_1.slurm}
	
	\lstinputlisting[language=bash, caption=\texttt{run\_greetings\_5\_8.slurm} used for running $40$ tasks across $5$ nodes:]{../run_greetings_5_8.slurm}

	\lstinputlisting[language=bash, caption=\texttt{greetings\_5\_8\_slurm.out} when running $40$ tasks across $5$ nodes:]{../greetings_5_8_slurm.out}
	
	\pagebreak
	\subsection{Exercise 2}
	The goal of this exercise is to modify the original \texttt{greetings.c} program so that process 0 can listen for messages from any process using any tag and compare the outputs.
	
	To do this, I simply changed the \texttt{source} and \texttt{tag} argument in the \texttt{MPI\_Recv} function to be \texttt{MPI\_ANY\_SOURCE} and \texttt{MPI\_ANY\_TAG} respectively and compiled using the same command used for exercise $1$. I also changed the name of my output and error files to greetings\_5\_8\_any\_slurm with file extensions .out and .err respectively. I used the same .slurm file for $5$ nodes, $8$ processes per node and \texttt{sbatch} command to run my code.
	
	After my code ran, my output file still had greeting messages from processes $1$-$39$ inclusive, but in a random order which is different from exercise $1$. This was to be expected because now that process $0$ listens for messages from any process it will print messages as they arrive which can be in any order in a parallel computing environment.
	
	\begin{lstlisting}[language=C, style=customc, caption=\texttt{MPI\_Recv} call is the only difference in code from exercise 1 to exercise 2:]
	MPI_Recv(message, 100, MPI_CHAR, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
	\end{lstlisting}
	
	\lstinputlisting[caption=Output when running $40$ processes over $5$ nodes but listening to all processes at once:]{../greetings_5_8_any_slurm.out}
	
	\pagebreak
	\subsection{Exercise 3}
	The purpose of this exercise is to see what happens when the parameters to \texttt{MPI\_Send} and \texttt{MPI\_Recv} are changed. For simplicity, I will only run my code on $1$ node and $5$ processes. To do this, I will update my .slurm file to have \texttt{nodes=1} and \texttt{ntasks-per-node=5} and rename the .slurm, .out, and .err files to follow my naming convention. After every change to \texttt{greetings.c} I will use the same compile command I stated earlier.
	I am starting with the same code I ended exercise $2$ with. I first changed the \texttt{datatype} in \texttt{MPI\_Send} to \texttt{MPI\_INT} because I know in C \texttt{int} and \texttt{char} are interchangeable for the most part. This resulted in an error that $104$ bytes were received but buffer size is $100$. TO attempt to remedy this, I increased \texttt{count} in \texttt{MPI\_Recv} to $200$. After this, messages were successfully received and I got the following output:
	\begin{lstlisting}[language=bash]
Greetings from process 3!
Greetings from process 4!
Greetings from process 1!
Greetings from process 2!
	\end{lstlisting}
	This appears to show that \texttt{MPI\_INT} datatypes are slightly larger than \texttt{MPI\_CHAR} datatypes.
	
	I next changed both \texttt{MPI\_Send} and \texttt{MPI\_Recv} to have a datatype of \texttt{MPI\_INT} and \texttt{count} back to $100$. This worked without error or warning and produced similar output to above, just in a different order. To try and find a lower bound for what \texttt{count} can be with my message, I set \texttt{datatype} back to \texttt{MPI\_CHAR} for both functions since I am working with \texttt{char} C types. I then found that for single digit process numbers, the greeting message is $25$ characters long plus the \texttt{null} terminating character that C adds, making the message $26$ \texttt{char}s long. I changed \texttt{count} to $26$ in \texttt{MPI\_Recv} since the length of the message is calculated in \texttt{MPI\_Send}. This also ran and surprisingly the messages were printed in order. I ran the same code twice more and got the same results. This is intriguing because I kept \texttt{MPI\_ANY\_SOURCE} and \texttt{MPI\_ANY\_TAG} in the \texttt{MPI\_Recv} function. I could run more tests, but for these brief experiments these results are enough to tell me it may not be a coincidence and could have something to do with how MPI pipelines data from processes. To see what happens when I reduce \texttt{count} to just $25$ in \texttt{MPI\_Recv}. This resulted in an error telling me $26$ bytes were received but the buffer size is only $25$ and caused the program to hang. These results suggest that for future programs, either an upper limit should be placed on messages constructed by sending processes, some method of predicting message size should be implemented by the receiving processes, or \texttt{count} should be assigned to some large value where it is only assumed no message will be constructed to be larger. Of all three, I believe the best to be setting an upper limit on messages constructed by sending processes.
	
	I increased \texttt{count} back to $26$ so I could start manipulating the \texttt{source} and \texttt{dest} parameters. I started by hard coding \texttt{source} to always be $2$ in \texttt{MPI\_Recv}. This caused only the greeting from process $2$ to be printed and the program to hang. This is because the receive function is inside a for-loop trying to get a message from process $2$ four times, once for each process. However in this case each process sends only a single message meaning process $0$ will only receive $1$ message from process $2$. I used \texttt{scancel} to cancel my job because I knew it would never finish. I changed \texttt{source} back to \texttt{MPI\_ANY\_SOURCE} for further testing. Changing \texttt{dest} in \texttt{MPI\_Send} to any value other than $0$ caused the program to hang without printing anything out so I had to cancel the job to stop it. This happened because process $0$ is waiting for $4$ messages, but all $4$ processes that send messages are sending them to process $2$, which is not expecting them.
	
	\lstinputlisting[language=bash, caption=\texttt{run\_greetings\_1\_5.slurm} used to run code after each modification]{../run_greetings_1_5.slurm}
	

\pagebreak
\section{Problem 2}
The purpose of this assignment is to write a program in which each process sends a greeting to the process with an $id$ one higher, or in the case of the process with the highest $id$, to process $0$ and have each process make sure to receive messages only from the process with $id$ one lower than their own, and in the case of process $0$ to receive messages only from the process with the highest $id$.
\subsection{Discussion of Relevant Issues and Solution Strategy}
The exact problem statement is that \textquotedblleft process $i$ sends a greeting to process $(i+1)\mod p$\textquotedblright where $p$ is the number of processes. If each process $i$ sends a message to process $i+1$ and receives from process $i-1$, process $p-1$ will be trying to send to process $p$ which does not exist, and process $0$ will be trying to receive from process $-1$ which does not exist either. The solution for sending is to have process $i$ send to process $(i+1)\mod p$. Thus processes $0$ through $p-2$ will send messages to process $i+1$ and process $p-1$ will send to process $0$. This means the code must calculate \texttt{dest} to be $(i+1)\% p$. The solution for receiving is for process $i$ to listen for a message from process $(i-1)\mod p$. On a computer, this will work for all processes except process $0$. The reason for this is that the computer's calculation of $-1\mod p$ is poorly defined across systems and programming languages. In particular, when testing this on the maya system using the \texttt{cc} compiler on the user node and computing the value $(0-1) \% p$ for any value $p > 1$ we get $-1$, meaning the error is not resolved. To solve this, we apply the following fact to our code:
$$ i-1 \equiv i-1+p \pmod{p} \Longrightarrow 1- \equiv -1+p \pmod{p}. $$
Thus, we compute \texttt{source} to be $()i-1+p) \% p $ and obtain the correct destination for every $i$ in $0\mathellipsis p-1$.

Another question regards the order of sending and receiving. It is useful to note that \texttt{MPI\_Send} and \texttt{MPI\_Recv} both block until the communication is complete. This means that if a process sends a message, it cannot do anything else until another process receives it; further, if a process is waiting for a message, it cannot do anything else until a message has been sent and it receives the message.

Consider if all processes chooses to receive a message first. All processes will block waiting for a message to be sent before sending their own message. This means no messages can be sent unless one process receives one and send its message; but no message can be sent and the program will hang indefinitely. Now consider every process sending its message to the correct destination and then trying to receive a message. Every process will wait for its message to be received before listening for a message to be sent to itself. But if all processes send first, none are listening for messages and the program blocks on send and hangs indefinitely. To solve this, have one process, say process $0$, choose to send first and all other processes listen first. Then process $1$ will immediately receive a message from process $0$ and send its own message to process $2$ which will then send a message to process $3$ and so on until process $p-2$ sends its message to process $p-1$ which will complete the circuit and send a message to process $0$ which has been listening ever since it sent its message to process $1$.

Because each process is given an $id$ independent of which processor it is on, it does not matter how many processors the program is run on, nor how many tasks per node are run. To test that our program works, we should have each process print its $id$ and when it sends, where it is sending to, and when it receives, where it received a message from. In addition, we should have process $0$ print the total number or processes running the program. We will know our program is correct if every process says it receives a message from a process with $id$ less than its own, except for process $0$ which should receive a message from $id=p-1$. If we verify that this is the case, we know every process $i$ has sent a message to process $(i+1)\%p$ that has been received correctly. To absolutely verify the sender of the message, we will use the \texttt{status} variable instead of only relying on the content of the message.

For this problem, I started with the exact code supplied by Pacheco, however I changed \texttt{my\_rank} into \texttt{id} and \texttt{p} to \texttt{np}. I changed the conditional tests to read \texttt{if (id == 0) \{...\} else \{...\}} and included in those blocks the strategy for sending and receiving messages described above. I tested my code with $15$ tasks distributed across $1$ node, then $3$ nodes, then $5$ nodes keeping the tasks per node the same on each node when using multiple nodes. See code listing at end of section for full source.

\subsection{Results}
Running the program on $1$ node with $15$ tasks yielded results that were mostly predictable: Process $1$ printed the greeting from process $0$ first, then process $2$ printed the greeting from process $1$, etc, until process $13$ printed the greeting from process $12$ and then process $0$ printed the greeting from process $14$ and finally process $14$ printed the greeting from process $13$. It was expected that process $0$ would print the greeting from process $14$ last because it would have to wait for process $14$ to receive a message, print it, and finally send its greeting to process $0$. The results indicate that the task scheduler on the node may have interrupted whichever process is in charge of directing data to standard out or to the slurm output file and allowed process $0$ to receive a message and print it first. A second run of the program was even more surprising: process $0$ managed to print the greeting from process $14$ immediately after process $1$ printed the greeting from process $0$ and before process $2$ could print the greeting from process $1$. From my limited knowledge of the inner workings of slurm, MPI, and the maya cluster, it appears that file I/O or buffering is being interrupted somewhere allowing process $0$ to interject the message it received from process $14$. Aside from this surprise, the code works as it should: every process $i$ is sends a message to process $(i+1)\%p$ and every process $i$  receives a message from process $(i-1+p) \% p$.

Running the program on $3$ nodes and $5$ tasks per node for a total of $15$ processes yielded the same results with process $0$ interjecting the greeting it received from process $14$ randomly between the outputs of the other processes. However running the program on $5$ nodes and $3$ tasks per node for a total of $15$ processes yielded increasingly out of order print statements. Ignoring this, each process still sent and received a message to and from the correct processes and thus the program is ran correctly.

Thus in all the tests, the code performed correctly, however the order of the output was somewhat surprising though the reason likely lies in the handling of I/O by the system rather than the implementation of the code itself.
\pagebreak
\lstinputlisting[language=bash, caption=\texttt{hi}Output when running on $3$ nodes with $5$ processes each:]{../greetings_3_5_slurm.out}
\lstinputlisting[language=C, style=customc, caption=\texttt{greetings\_3.c} source code for Problem 2.]{../snapshots/greetings_3.c}
\pagebreak

\section{Problem 3}
This problem discusses the program from Chapter 4 of Pacheco designed to calculate an approximation of a definite integral using trapezoids.
\subsection{Verification of Original Code}
The purpose of this section is to verify that the code given works for me and gives at least a close approximation of the actual value of the integral, as well as to identify any possible shortcomings in the code. To compile, I used \texttt{mpiicc trap.c -o trap}. For my first test, I ran the program on $2$ nodes with $4$ tasks per node to ensure the total number or processes was a divisor of $n=1024$, the number of trapezoids used in the approximation. The run completed quickly without error and gave an estimation of 0.333333 which is fairly close to the actual value of the integral which is $\frac{1}{3}$.

Potential shortcomings of this code arise from the author's attempt at a simple example. To begin, the code uses \texttt{my\_rank} and \texttt{p} to store the process's id and the total number of processes. Another misnamed variable is \texttt{integral}, which is actually used to accumulate the approximation for the integral. This code does not allow the user to specify command line arguments to set the bounds of the integral to approximate or the number or trapezoids, making it a very inflexible program. The program also relies on the number of processes dividing the number or trapezoids used in the approximation. The program's assumptions could yield an answer that leaves out approximations of part of the function over the bounds, for example if $np=3$ then one trapezoid would be left out of the approximation. The program also only uses \texttt{float}s for real values, which do not have nearly enough precision to accurately make an approximation for a very large number of trapezoids. Another shortcoming comes from the fact that it does not print information such as the step size, the true value of the integral, the true error, the acceptable error, or the number of intervals used. When it does print the answer, it does not format the answer to leave out insignificant bits.

\subsection{Fixing the Shortcomings}
I first began by converting all \texttt{float} types to \texttt{double} types to increase the amount of precision and renaming variables to what they actually represent, namely \texttt{my\_rank} becomes \texttt{id}; \texttt{p} becomes \texttt{np}; \texttt{integral} becomes \texttt{approximation}. When I added the detailed diagnostic print statements, I used a format of \texttt{\%24.16e} so I could have a field width of 24 digits and 16 digits after the decimal place of a number with type \texttt{double}. I chose this number because the \texttt{double} type has $16$ bits of precision. I also printed the dependent variables, the ones that are not calculated as part of the approximation but are rather used in the approximation such as \texttt{a, b, n, np, h, local\_h} etc, first with an indent and the calculated values and true value of the integral underneath, unindented. This is just for aesthetic purposes and easier distinction.

To install command line arguments for $a$ and $b$ I just used the \texttt{atof()} function on elements $1$ and $2$ of the \texttt{argv} array which converts \texttt{char \*} types to \texttt{double}s. To get the value of $n$, I used \texttt{atof()} and casted that to an \texttt{int}. The reason for this is that using \texttt{atof()} allows users to use exponential notation to set the value of $n$ if they want it very large, but using \texttt{atoi()} cannot parse exponential notation and would otherwise throw an error.

For error checking, I first checked if the value of $n$ was less than or equal to $0$. The trapezoidal method of approximating an integral requires at least one trapezoid so it would be a user error to specify any less than $1$. I also included a check to see if it was process $0$ to make sure I only print an error message from process $0$. After printing, I call \texttt{MPI\_Abort} with an error code of $-1$. In the state the code is now, it would also be an error for the number of processes to not divide the number of trapezoids. To check for this case, I computed the remainder of $\frac{n}{p}$ using \texttt{n \% np} and if it was non-zero I checked if the \texttt{id} was $0$. If both were true, I printed an error message and again called \texttt{MPI\_Abort} with an error code of $-1$.

\subsection{Source of an Inaccuracy}
When the number of processes does not divide the number of trapezoids used for the approximation, the current state of the program will only calculate the first $ \lfloor\frac{n}{p}\rfloor \cdot p $ trapezoids. This is due to the fact that the code calculates a value \texttt{local\_n} that represents how many trapezoids each of the $p$ processes will calculate as part of the approximation of the integral. It does this using the statement \texttt{local\_n = n/np;} which is an integer division, meaning it truncates anything after the decimal point. For example, if we have $3$ processes and $1024$ trapezoids, process $0$will process trapezoids $0$ inclusive up to $341$ exclusive; process $1$ will process trapezoids $341$ inclusive up to $642$ exclusive; finally process $2$ will process trapezoids $642$ inclusive through $983$ exclusive. This leaves the last $40$ trapezoids unaccounted for. Removing the error checking I did related to this, the program yields an error of $ -9.7545034562546151\times10^{-04}$ which can be significant in scientific or engineering applications.

To solve this problem, I propose each process $i$ is assigned to process trapezoids $i+k*p$ for all integer $k$ including $0$ such that $i+k*p < n$. To show that any process $i$ will have at most $1$ more trapezoid to process than any other process $j$, let us assume there exists a process $i$ that has two more trapezoids to process than a process $j$. This means process $i$ is assigned trapezoids $i, i+p, i+2p,\mathellipsis, i+k*p$ and process $j$ is assigned trapezoids $j, j+p, j+2p, \mathellipsis, j+(k-2)*p$. $ (i+k*p)-(j+(k-2)*p) = (i-j)+(k-(k-2))*p = (i-j)+2p $. Since $i<p$ and $j<p$, we have $-p < i-j < p \Longrightarrow p < i-j +2p < 3p $. This means the difference between the highest trapezoid of process $i$ is at least $p$ trapezoids away from the highest trapezoid of process $j$ meaning $j+(k-1)*p < i+k*p < n$ meaning process $j$ must also be assigned trapezoid $j+(k-1)*p$ and that process $i$ cannot be assigned trapezoid $i+(k+1)*p$ because $i+(k+1)*p > n$. Thus process $i$ can have at most $1$ more trapezoid than a process $j$ and will only have $1$ more trapezoid than a process $j$ if $i < j$ and $p$ does not divide $n$.

To implement my algorithm, I got rid of all instances of \texttt{local\_n} and \texttt{local\_h} and only kept \texttt{local\_a} and \texttt{local\_b} inside of the \texttt{Trap} function. I modified the function declaration of \texttt{Trap} to be \texttt{double Trap(int n, double h, double a, int id, int np)}. I modified the function so that it increments $k$ in a loop from $0$ upwards while $id+k*np < n$. Inside this loop I calculate which trapezoid needs to be processed and compute \texttt{local\_a}, the x-coordinate of the leftmost base of the trapezoid, to be \texttt{trapezoid*h+a} and \texttt{local\_b}, the x-coordinate of the rightmost base of the trapezoid, to be \texttt{()trapezoid+1)*h+a}. I then use the formula for the area of a trapezoid and add that to the approximation the current process is doing. Once the loop is finished, it sends the approximation to process $0$ which sums all approximations from all processes and prints the information I deemed useful earlier.

My algorithm has used all trapezoids in the approximation when $np$ divides $n$, $np=n$, and $np > n$, which is all scenarios other than when $n$ or $np$ is zero, however those cases have no real mathematical meaning.

\begin{table}
\centering
\begin{tabular}{|c|c|c|}
\hline
	& 1 node & 2 nodes  \\ [0.5ex] 
 \hline
 1 process per node & 1.5974044799804688e-05 & 2.9087066650390625e-05   \\ 
 \hline
 2 processes per node & 3.1948089599609375e-05 & 3.0040740966796875e-05   \\
 \hline
4 processes per node & 2.5033950805664062e-05 & 2.9087066650390625e-05   \\
 \hline
8 processes per node & 2.3126602172851562e-05 & 4.1961669921875000e-05   \\
 \hline
16 processes per node &  1.8191337585449219e-04 & 1.7309188842773438e-04   \\ [1ex] 
 \hline
\end{tabular}
\caption{Times(in seconds) of runs on hpcf2013 development nodes approximating the integral of $x^2$ from $0$ to $1$ using $8192$ trapezoids.}
\end{table}

\begin{table}
\centering
\begin{tabular}{|c|c|c|}
\hline
	& 1 node & 2 nodes  \\ [0.5ex] 
 \hline
 1 process per node & 3.0303001403808594e-04 & 1.5997886657714844e-04   \\ 
 \hline
 2 processes per node & 1.6188621520996094e-04 & 1.1110305786132812e-04   \\
 \hline
4 processes per node & 1.0895729064941406e-04 & 7.0095062255859375e-05   \\
 \hline
8 processes per node & 6.8902969360351562e-05 & 6.0081481933593750e-05   \\
 \hline
16 processes per node & 8.2969665527343750e-05 & 1.9311904907226562e-04   \\ [1ex] 
 \hline
\end{tabular}
\caption{Times in seconds of runs on hpcf2013 development nodes approximating the integral of $\pi sin(\pi*x^2)$ from $0$ to $1$ using $8192$ trapezoids.}
\end{table}

My tests ran without program errors. For the approximation of the integral of $f(x)=x^2$ from $0$ to $1$ using $8192$ trapezoids, my programs all reported the same answer which was $2.4835268841449931e-09$ off of the true value. For the approximation of the integral of $g(x)=\pi sin(\pi*x^2)$ from $0$ to $1$ using $8192$ trapezoids, my programs all reported the same answer which was $-2.4511426666151692e-08$ off of the true value. Both of these actual errors are extremely small leading me to conclude that my programs resulted in very good approximations.

To make my timing accurate, I placed an \texttt{MPI\_Barrier} immediatly after my error checks and getting command line args and right before process $0$ prints results. I did this so that I would have two clearly defined points where all processes have to be at the same time , regardless of which gets there first and last. Thus I was able to use \texttt{MPI\_Wtime} to record the times when all processes were at the same point before they began computing trapezoid areas and right after they finished and process $0$ had aggregated the results. The time I calculated is only the time required for the whole program to do the computations involved and share their results, not any of the setup or printing results since both of those can take a long time without doing any work to reach an answer.

I notice a general trend of decreasing speeds as the per node increases when using only $1$ node, however the approximation for $f(x)=x^2$ became slower when it was distributed over $2$ nodes in all but $1$ case. This could be caused by network latencies, but more tests are required for anything conclusive. I ensure that my code achieved close enough answers by validating my algorithms mathematically by hand and then reviewing them once I had put them in code. I also included the true value of the integral and the difference between the true answer and the approximation in the print outs. I did not encounter this, but one limitation of my code is dealing with values exceeding 32 bits in size. This is due to my choice of double and int rather than other datatypes or using software libraries designed for big arithmetic.

Looking at the data for runs of the second function, I notice that some of them are slower than the first function by about a factor of $10$, though not all of them are.  Without performing serial tests of both functions and comparing that to the parallel runs, it is hard to tell which function had a greater speedup by going parallel.


\end{document}












































