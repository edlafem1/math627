\documentclass[11pt]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage[table]{xcolor}
 
\usepackage{xcolor}
\usepackage{listings}

\lstdefinestyle{customc}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  frame=L,
  xleftmargin=\parindent,
  language=C,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\bfseries\color{green!40!black},
  commentstyle=\ttfamily\color{purple!40!black},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
}
\lstset{escapechar=@,style=customc}

\usepackage{graphicx}

\author{Edward LaFemina \\
		\it{University of Maryland, Baltimore County}}

\title{A Comparison of Blocking and Non-Blocking Communications}
\date{\today}
\begin{document}
\maketitle

\begin{abstract}
This paper will describe the results of solving the Poisson problem with homogeneous Dirichlet boundary by the discretized finite difference method conditions on the maya 2013 cluster. The specific goal is to compare blocking and non-blocking communication between processes using MPI functions. We first perform the conjugate gradient method of solving this problem using Matlab. We then use a serial matrix-vector product function and serial dot-product function in a conjugate gradient function provided by Dr. Gobbert. We then move to parallelize our computation by writing a parallel matrix-vector product function. Because we must divide the matrix, we must communicate some points of our domain to other processes and receive some in return. It is here that we make our comparison. Due to high utilization of maya 2013, we were not able to obtain any results at this time and cannot make a recommendation  on blocking vs non-blocking communication.
\end{abstract}
\section{Introduction}
The MPI programming package is used to facilitate communications between processes running in parallel on potentially massively distributed sytsems. Basic send and receive operations can be either blocking or non-blocking. Blocking communications causes the calling process to stall while waiting for a message to be delivered to another process, or waiting on a message from another process. We will be comparing performance of Matlab, serial C code, parallel C code with blocking communications, and parallel C code with non-blocking communications. To do so, we will work on a classical test problem given by the numerical solution with finite differences for the Poisson problem with homogeneous Dirichlet boundary conditions.

All computation is performed on any of the 72 nodes in maya 2013, each with two eight-core 2.6 GHz Intel E5-2650v2 Ivy Bridge CPUs with 64GB of memory per node. Nodes are connected by a high-speed quad-data rage InfiniBand network. The Intel C compiler version 14.0 is used with options \texttt{-std=c99 -Wall -O3} with Intel MPI version 4.1. When running tests, we required exclusive access to a node, even if we only used a single core.

\section{Problem Statement}
The Poisson problem with homogeneous Dirichlet boundary conditions, given as
\begin{align*}
&-\triangle u=f &\text{in}\hspace{1em}& \Omega\\
&u=0            &\text{on}\hspace{1em}& \partial\Omega
\end{align*}
Here, $\partial\Omega$ denotes teh boundry of the domain $\Omega$ and the Laplace operator is defined as
$$\triangle u=\frac{\partial ^2 u}{\partial x^2}+\frac{\partial ^2 u}{\partial y^2}.$$
We will consider the problem on the two dimensional unit squre $\Omega=(0,1)\times (0,1)\subset \mathbb{R}^2$. Thus, we can rewrite the boundry conditions previously stated as
\begin{align*}
&-\frac{\partial ^2 u}{\partial x^2}-\frac{\partial ^2 u}{\partial y^2}=f(x,y)&\\
&u(0,y)=u(1,y)=u(x,0)=u(x,1)=0 &
\end{align*}
$\text{for}\hspace{1em} 0<x<1,\hspace{1em} 0<y<1$ where $f$ is given as
$$f(x,y)=-2\pi ^2 cos(2\pi x)sin^2 (\pi y)-2\pi ^2 sin^2(\pi x)cos(2\pi y).$$
This problem is designed to yield the true solution
$$u(x,y)=sin^2 (\pi x)sin^2 (\pi y).$$
\section{Numerical Method}
To solve the Poisson problem with homogeneous Dirichlet boundry conditions, we have discretized it on a mesh of points $\Omega_h ={(x_i, y_j)=(ih,jh),i,j=0,...N+1}$ with uniform mesh width $h=\frac{1}{N+1}$. This yields equations that ban be organized into a linear system $Au=b$ of $N^2$ equations for approximations of $u_{i,j}$[1\S 3.2]. Because of our boundry conditions, we know there are exactly $N^2$ unknowns. From [5,14] we understand the matrix $A$ is symmetric and positive definite, implying the linear system has a unique solution and also guarantees that the iterative conjugate gradient method converges. For details about how the matrix $A$ and vector $b$ are constructed, see HCPF-2012-15\S3.2[1].

To confirm the convergence of the finite difference method, we use the finite difference error. This is defined as the difference between the true solution $u(x,y)$ and the calculated numeric solution defined on the mesh points $u_x(x_i,y_i)=u_{i,j}$. We expect that as $N$ gets larger and $h=\frac{1}{N+1}$ gets smaller the finite difference error denoted $||u-u_h||$ will become smaller. We can then expect the ratio of errors on consecutively refined meshes to approach $4$ by looking at Eqn. 3.7 from HPCF-2012-15[1]. If the ratio we compute approaches $4$, we can be confident that our approximations are converging to the true solution.

\section{Results}
\subsection{Matlab}
Matlab code using the conjugate gradient method to solve the Poisson problem with homogeneous Dirichlet boundry conditions was provided by Dr. Gobbert. Below are results from running the conjugate gradient method in Matlab at various values of $N$. Notice that as $N$ increases, the ratio of the finite difference error converges to $4$, indicating that our approximation converges to the true solution as we increase $N$. We will time all runs from here on by placing calls to \texttt{MPI\_Barrier} and \texttt{MPI\_Wtime} before and after the call to the conjugate gradient function. This will ensure that all processes are at those spots before the conjugate gradient method starts and when it is finished.
\begin{table}[!htbp]
\makebox[\linewidth]{
\begin{tabular}{ |c|c|c|c|c|c|  }
\hline
\multicolumn{6}{|c|}{Convergence Study of Matlab Code}\\
\hline
N & DOF & $||u-u_h||$ & Ratio & \#iter & Time(s) \\
\hline
32   & 1024    & 3.012e-3 & N/A    & 48   & 0.01   \\
64   & 4096    & 7.781e-4 & 3.8719 & 96   & 0.05   \\
128  & 16384   & 1.976e-4 & 3.9368 & 192  & 0.13   \\
256  & 65536   & 4.979e-5 & 3.9691 & 387  & 1.03   \\
512  & 262144  & 1.249e-5 & 3.9857 & 783  & 6.49   \\
1024 & 1048576 & 3.126e-6 & 3.9960 & 1581 & 51.58  \\
2048 & 4194304 & 7.801e-7 & 4.0075 & 3192 & 426.84 \\
\hline
\end{tabular}
}
\end{table}
\pagebreak
\subsection{Serial C}
Unlike Matlab, C has no built in functions to perform matrix operations or the conjugate gradient method. To best follow the Matlab code, we reused a serial matrix-vector function we developed previously as well as a serial dot product function developed previously. The code to perform the conjugate gradient method was supplied by Dr. Gobbert. All that remained for us to do is setup the $b$ vector by calculating $h^2 * f(x_i, y_i)$. Below is a table representing a convergence study done on the serial code to determine correctness of results and also display expected vs actual memory usage. Our estimates are based off the fact that the program needs four vectors of doubles, each of length $N^2$. All tests were run using one node with one process per node. In all cases we were well below the maximum number of iterations we specified(99999) and the number of iterations taken is the same as the number taken by the Matlab implementation of our code. It should also be noted that the finite difference norms and ratios between them are the same as those in the Matlab code. This indicates that our C code functions properly and attains the correct result. The ratio of finite difference norms approached $4$, indicating that our approximations do converge to the true solution.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SOMETHING IS WRONG HERE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!htbp]
\makebox[\linewidth]{
\begin{tabular}{ |c|c|c|c|c|c|c|c|  }
\hline
\multicolumn{8}{|c|}{Convergence Study of C Code}\\
\hline
N & DOF & $||u-u_h||$ & Ratio & \#iter & Time(s) & Predicted Memory(GB) & Actual Memory(GB) \\
\hline
32    & 1024    & $3.0127E-03$ & N/A    & 48    & 0.0123     & $< 1$ & $< 1$  \\
64    & 4096    & $7.7811E-04$ & 3.8719 & 96    & 0.0030     & $< 1$ & $< 1$  \\
128   & 16384   & $1.9765E-04$ & 3.9368 & 192   & 0.0321     & $< 1$ & $< 1$  \\
256   & 65536   & $4.9797E-05$ & 3.9690 & 387   & 0.1688     & $< 1$ & $< 1$  \\
512   & 262144  & $1.2494E-05$ & 3.9856 & 783   & 1.2435     & $< 1$ & $< 1$  \\
1024  & 1048576 & $3.1266E-06$ & 3.9961 & 1581  & 12.4322    & $< 1$ & $< 1$  \\
2048  & 4194304 & $7.8019E-07$ & 4.0075 & 3192  & 124.3381   & $< 1$ & $< 1$  \\
4096  & 4194304 & $1.9366E-07$ & 4.0287 & 6542  & 1002.8323  & $< 1$ & $< 1$  \\           
8192  & 4194304 & $4.7376E-08$ & 4.0877 & 13033 & 8090.2605  &   2   & 2.03   \\
16384 & 4194304 & $1.1543E-08$ & 4.1045 & 26316 & 66648.1740 &   8   & 8.03   \\
32768 & 4194304 &  ----------  & ------ & ----- & ---------- &  32   & ------ \\
\hline
\multicolumn{8}{|c|}{The case $N=32768$ timed out when running one maya with \texttt{qos=medium}\\
\end{tabular}
}
\end{table}

\subsection{Parallel C - Blocking}
Progressing from serial code, we parallelized our matrix-vector product function. Because we used a colunn major storage format, splitting the matrix $A$ over several processes by dividing it into blocks of columns is equivalent to dividing the domain $\Omega$ into subdomains horizontally. This meant that the computation at the points on the top and bottom rows of our subdomain relied on points stored on another process, and possibly another processor. To handle this, we first sent the points we stored locally to processes that would need them and waited to receive the points we needed. Because we used blocking communication for this stage, our process waited for the communicaton to stop and then moved on to perform computation at the interior points of our subdomain and lastly performed computation on the boundry points. We again verified that our finite difference norm and ratio between them matched that of the serial C code and the Matlab code to be sure that our parallel C code achieved the same result.
Below is a table of times(in seconds) to complete the conjugate gradient method for every possible combination of processors per node and number of processors for $1,2,4,8,$ and $15$ processes per node and $1,2,4,8,16,32,$ and $64$ nodes at all values of $N=1024, 2048, 4096, 8192, 16384, 32768$. Note, not all computations completed before this report was completed.

\begin{table}[!htbp]
\makebox[\linewidth]{
\begin{tabular}{ |c|c|c|c|c|c| }
\hline
\multicolumn{6}{|c|}{(a) Mesh resolution N  $\times$  N = 1024 $\times$ 1024, system dimension 1,048,576}\\
\hline
 & 1 node & 2 nodes & 4 noes & 8 nodes & 16 nodes \\
\hline
1 process per node  & 12.2329 & 5.1400 & -----  & -----   & 0.9088 \\
2 process per node  & -----   & -----  & -----  & 0.8503  & 0.5849 \\
4 process per node  & -----   & -----  & -----  & 0.5260  & -----  \\
8 process per node  & 3.4756  & -----  & -----  & 0.4268  & -----  \\
16 process per node & 0.8110  & -----  & 0.5435 & 0.6863  & 0.9141 \\
\hline
\hline
\multicolumn{6}{|c|}{(b) Mesh resolution N $\times$ N = 2048 $\times$ 2048, system dimension 4,194,304}\\
\hline
 & 1 node & 2 nodes & 4 noes & 8 nodes & 16 nodes \\
\hline
1 process per node  & 124.3657 & ----- & -----  & -----   & 5.9926 \\
2 process per node  & -----    & ----- & -----  & 5.7192  & 3.2807 \\
4 process per node  & -----    & ----- & -----  & 3.6034  & 2.0095 \\
8 process per node  & 47.1761  & ----- & -----  & 2.0780  & -----  \\
16 process per node & 23.3534  & ----- & 2.4149 & 1.8251  & 2.0300 \\
\hline
\hline
\multicolumn{6}{|c|}{(c) Mesh resolution N $\times$ N = 4096 $\times$ 4096, system dimension 16,777,216}\\
\hline
 & 1 node & 2 nodes & 4 noes & 8 nodes & 16 nodes \\
\hline
1 process per node  & ----- & 507.2436 & -----   & -----   & 62.1962 \\
2 process per node  & ----- & 281.3695 & -----   & -----   & 33.5026 \\
4 process per node  & ----- & -----    & -----   & 50.7268 & 22.4481 \\
8 process per node  & ----- & -----    & -----   & 48.0966 & 15.6491 \\
16 process per node & ----- & -----    & 49.7290 & 18.8387 & 6.7330  \\
\hline
\hline
\multicolumn{6}{|c|}{(d) Mesh resolution N $\times$ N = 8192 $\times$ 8192, system dimension 67,108,864}\\
\hline
 & 1 node & 2 nodes & 4 noes & 8 nodes & 16 nodes \\
\hline
1 process per node  & -----     & 4081.6881 & ----- & -----    & -----    \\
2 process per node  & -----     & 2286.0044 & ----- & 603.5596 & -----    \\
4 process per node  & -----     & -----     & ----- & 421.3476 & -----    \\
8 process per node  & 2995.3809 & -----     & ----- & 208.1165 & 232.6030 \\
16 process per node & 1522.5461 & -----     & ----- & 240.9191 & 180.2524 \\
\hline
\hline
\multicolumn{6}{|c|}{(e) Mesh resolution N $\times$ N = 16384 $\times$ 16384, system dimension 268,435,456}\\
\hline
 & 1 node & 2 nodes & 4 noes & 8 nodes & 16 nodes \\
\hline
1 process per node  & -----      & 33475.99647 & -----     & -----     & 4539.0598 \\
2 process per node  & -----      & -----       & -----     & -----     & 2515.7832 \\
4 process per node  & -----      & -----       & -----     & 3397.3418 & 1778.0936 \\
8 process per node  & 24379.6003 & -----       & -----     & -----     & 1429.0208 \\
16 process per node & 12236.3626 & -----       & 3201.5184 & -----     & 1015.491  \\
\hline
\hline
\multicolumn{6}{|c|}{(f) Mesh resolution N $\times$ N = 32768 $\times$ 32768, system dimension 1,073,741,824}\\
\hline
 & 1 node & 2 nodes & 4 noes & 8 nodes & 16 nodes \\
\hline
1 process per node  & ----- & ----- & -----      & -----      & 36179.7306 \\
2 process per node  & ----- & ----- & 76566.5580 & -----      & 20552.7364 \\
4 process per node  & ----- & ----- & -----      & -----      & 14161.6356 \\
8 process per node  & ----- & ----- & -----      & 25487.6948 & 13160.3221 \\
16 process per node & ----- & ----- & 25619.7192 & 13480.9432 & 7862.5643  \\
\hline
\end{tabular}
}
\end{table}
\pagebreak
\pagebreak
\subsection{Parallel C - Non-Blocking}
To change from blocking to non-blocking communication, we simply had to change the function names we were already using to the ones that indicated non-blocking communication and add a different set of parameters that are MPI specific and do not affect any of the computations. Because we start the communication before any computation, the non-blocking nature of the communication allows the program to start computations on interior points while it waits for the communication to finish. After the interior point computations are complete, we make sure the communications are finished by using the MPI\_Waitall function to block for a short time while any ongoing communication is finished. Below is a table of times(in seconds) it takes for our conjugate gradient method to finish for the same run configuration as above, but this time using non-blocking communication functions.
\begin{table}[!htbp]
\makebox[\linewidth]{
\begin{tabular}{ |c|c|c|c|c|c| }
\hline
\multicolumn{6}{|c|}{(a) Mesh resolution N  $\times$  N = 1024 $\times$ 1024, system dimension 1,048,576}\\
\hline
 & 1 node & 2 nodes & 4 noes & 8 nodes & 16 nodes \\
\hline
1 process per node  & 11.6632 & 4.8071 & 2.4011 & 1.2593 & 0.7549 \\
2 process per node  & 6.2762  & 2.4465 & 1.2842 & 0.7337 & 0.4288 \\
4 process per node  & 4.4103  & 1.3596 & 0.8562 & -----  & -----  \\
8 process per node  & 3.3875  & -----  & -----  & 0.4105 & 0.4273 \\
16 process per node & 0.7083  & -----  & 0.4701 & -----  & -----  \\
\hline
\hline
\multicolumn{6}{|c|}{(b) Mesh resolution N $\times$ N = 2048 $\times$ 2048, system dimension 4,194,304}\\
\hline
 & 1 node & 2 nodes & 4 noes & 8 nodes & 16 nodes \\
\hline
1 process per node  & ----- & ----- & ----- & ----- & ----- \\
2 process per node  & ----- & ----- & ----- & ----- & ----- \\
4 process per node  & ----- & ----- & ----- & ----- & ----- \\
8 process per node  & ----- & ----- & ----- & ----- & ----- \\
16 process per node & ----- & ----- & ----- & ----- & ----- \\
\hline
\hline
\multicolumn{6}{|c|}{(c) Mesh resolution N $\times$ N = 4096 $\times$ 4096, system dimension 16,777,216}\\
\hline
 & 1 node & 2 nodes & 4 noes & 8 nodes & 16 nodes \\
\hline
1 process per node  & ----- & ----- & ----- & ----- & ----- \\
2 process per node  & ----- & ----- & ----- & ----- & ----- \\
4 process per node  & ----- & ----- & ----- & ----- & ----- \\
8 process per node  & ----- & ----- & ----- & ----- & ----- \\
16 process per node & ----- & ----- & ----- & ----- & ----- \\
\hline
\hline
\multicolumn{6}{|c|}{(d) Mesh resolution N $\times$ N = 8192 $\times$ 8192, system dimension 67,108,864}\\
\hline
 & 1 node & 2 nodes & 4 noes & 8 nodes & 16 nodes \\
\hline
1 process per node  & ----- & ----- & ----- & ----- & ----- \\
2 process per node  & ----- & ----- & ----- & ----- & ----- \\
4 process per node  & ----- & ----- & ----- & ----- & ----- \\
8 process per node  & ----- & ----- & ----- & ----- & ----- \\
16 process per node & ----- & ----- & ----- & ----- & ----- \\
\hline
\hline
\multicolumn{6}{|c|}{(e) Mesh resolution N $\times$ N = 16384 $\times$ 16384, system dimension 268,435,456}\\
\hline
 & 1 node & 2 nodes & 4 noes & 8 nodes & 16 nodes \\
\hline
1 process per node  & ----- & ----- & ----- & ----- & ----- \\
2 process per node  & ----- & ----- & ----- & ----- & ----- \\
4 process per node  & ----- & ----- & ----- & ----- & ----- \\
8 process per node  & ----- & ----- & ----- & ----- & ----- \\
16 process per node & ----- & ----- & ----- & ----- & ----- \\
\hline
\end{tabular}
}
\end{table}

\begin{table}[!htbp]
\makebox[\linewidth]{
\begin{tabular}{ |c|c|c|c|c|c| }
\hline
\multicolumn{6}{|c|}{(f) Mesh resolution N $\times$ N = 32768 $\times$ 32768, system dimension 1,073,741,824}\\
\hline
 & 1 node & 2 nodes & 4 noes & 8 nodes & 16 nodes \\
\hline
1 process per node  & ----- & ----- & ----- & ----- & ----- \\
2 process per node  & ----- & ----- & ----- & ----- & ----- \\
4 process per node  & ----- & ----- & ----- & ----- & ----- \\
8 process per node  & ----- & ----- & ----- & ----- & ----- \\
16 process per node & ----- & ----- & ----- & ----- & ----- \\
\hline
\end{tabular}
}
\end{table}
\pagebreak
\section{Conclusions}
Due to high utilization of the maya cluster, very little data was able to be collected and thus we cannot draw any conclusions at this time.
\section*{References}

$\left[2\right]$  Ecaterina Coman, Matthew Brewster, et al. A Comparative Evaluation of Matlab, Octave, FreeMat, Scilab, R, and IDL on Tara. Technical Report HPCF–2012-15, UMBC High Performance Computing Facility, University of Maryland, Baltimore County, 2012.\\
$\left[2\right]$  Samuel Khuvis and Matthias Gobbert. Parallel Performance Studies for an Elliptic Test Problem on the Cluster maya. Technical Report HPCF–2015-6, UMBC High Performance Computing Facility, University of Maryland, Baltimore County, 2015.
\end{document}
























