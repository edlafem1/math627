% First we choose the document class (can be book, report, article etc.)
\documentclass[11pt]{article}
\usepackage{xcolor}
\usepackage{listings}

\lstdefinestyle{customc}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  frame=L,
  xleftmargin=\parindent,
  language=C,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\bfseries\color{green!40!black},
  commentstyle=\ttfamily\color{purple!40!black},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
}
\lstset{escapechar=@,style=customc}

\author{Edward LaFemina \\
		\it{University of Maryland, Baltimore County}}
\title{Math 627 HW 2}
\date{\today}

\begin{document}
\maketitle
\tableofcontents

\pagebreak
\section{Problem 1}
I will be completing exercises 1, 2, and 3 in Chapter 3 of Pacheco.
	\subsection{Exercise 1}
	This goal of this exercise is to run the given \texttt{greetings.c} program written by Pacheco first on one processor, then on as many as possible and compare the outputs.
	
	I first downloaded the greetings.c source code from the Pacheco website and copied and pasted it onto the maya cluster in my math627 directory. To compile, I used the command \texttt{mpiicc greetings.c -o greetings}. To run my program on only one process, I created a file called \texttt{run\_greetings\_1\_1.slurm} which contains directives used by slurm to run my code. I set the job-name to greetings, the partition to develop, nodes to $1$, ntasks-per-node to $1$, and, adopting a naming convention of \textless job-name\textgreater\_\textless nodes\textgreater\_\textless processes per node\textgreater\_slurm, I named my output and error files with file extensions .out and .err respectively. I then ran my code using the command \texttt{sbatch run\_greetings\_1\_1.slurm}.
	
	My code ran without error and the output and error files were both empty. I believe this to be correct because the code copied from Pacheco only listens for messages on process 0 without creating any of its own. Thus, no messages are sent and none can be received so the output remains blank. Keeping on the development partition to avoid interfering with research projects, a table on the maya resource page states there are 6 nodes I can run on and a paragraph of text above the table says I can only use $5$ nodes, however I found that though I can only use $5$ nodes, the ones I can use are slightly different than those listed in the paragraph. I can run my code on $5$ nodes, $2$ hpcf2013, $1$ hpcf2010, and $2$ hpcf2009 nodes, for a total of $48$ processes. Because slurm limits the ntasks-per-node value to the minimum of the number or processes the requested nodes can handle, I will only run my code on $5$ nodes with $8$ processes each for a total of $40$ processes. To do this, I will make a new \texttt{.slurm} file following my naming convention and setting nodes to $5$ and ntasks-per-node to $8$.
	
	When this completes, my .out file is filled with the greeting from processes $1$ through $39$ inclusive and in order, least to greatest. This is expected because of the way process $0$ is listening for messages. It listens first for messages from process $1$, then process $2$, etc up to the last process, process $39$.
	\pagebreak

	\lstinputlisting[language=C, style=customc, caption=The C source code I used\, taken straight from the Pacheco website:]{../snapshots/greetings_1.c}
	
	\lstinputlisting[language=bash, caption=\texttt{run\_greetings\_1\_1.slurm} used for running on a single process:]{../run_greetings_1_1.slurm}
	
	\lstinputlisting[language=bash, caption=\texttt{run\_greetings\_5\_8.slurm} used for running $40$ tasks across $5$ nodes:]{../run_greetings_5_8.slurm}

	\lstinputlisting[language=bash, caption=\texttt{greetings\_5\_8\_slurm.out} when running $40$ tasks across $5$ nodes:]{../greetings_5_8_slurm.out}
	
	\pagebreak
	\subsection{Exercise 2}
	The goal of this exercise is to modify the original \texttt{greetings.c} program so that process 0 can listen for messages from any process using any tag and compare the outputs.
	
	To do this, I simply changed the \texttt{source} and \texttt{tag} argument in the \texttt{MPI\_Recv} function to be \texttt{MPI\_ANY\_SOURCE} and \texttt{MPI\_ANY\_TAG} respectively and compiled using the same command used for exercise $1$. I also changed the name of my output and error files to greetings\_5\_8\_any\_slurm with file extensions .out and .err respectively. I used the same .slurm file for $5$ nodes, $8$ processes per node and \texttt{sbatch} command to run my code.
	
	After my code ran, my output file still had greeting messages from processes $1$-$39$ inclusive, but in a random order which is different from exercise $1$. This was to be expected because now that process $0$ listens for messages from any process it will print messages as they arrive which can be in any order in a parallel computing environment.
	
	\begin{lstlisting}[language=C, style=customc, caption=\texttt{MPI\_Recv} call is the only difference in code from exercise 1 to exercise 2:]
	MPI_Recv(message, 100, MPI_CHAR, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
	\end{lstlisting}
	
	\lstinputlisting[caption=Output when running $40$ processes over $5$ nodes but listening to all processes at once:]{../greetings_5_8_any_slurm.out}
	
	\pagebreak
	\subsection{Exercise 3}
	The purpose of this exercise is to see what happens when the parameters to \texttt{MPI\_Send} and \texttt{MPI\_Recv} are changed. For simplicity, I will only run my code on $1$ node and $5$ processes. To do this, I will update my .slurm file to have \texttt{nodes=1} and \texttt{ntasks-per-node=5} and rename the .slurm, .out, and .err files to follow my naming convention. After every change to \texttt{greetings.c} I will use the same compile command I stated earlier.
	I am starting with the same code I ended exercise $2$ with. I first changed the \texttt{datatype} in \texttt{MPI\_Send} to \texttt{MPI\_INT} because I know in C \texttt{int} and \texttt{char} are interchangeable for the most part. This resulted in an error that $104$ bytes were received but buffer size is $100$. TO attempt to remedy this, I increased \texttt{count} in \texttt{MPI\_Recv} to $200$. After this, messages were successfully received and I got the following output:
	\begin{lstlisting}[language=bash]
Greetings from process 3!
Greetings from process 4!
Greetings from process 1!
Greetings from process 2!
	\end{lstlisting}
	This appears to show that \texttt{MPI\_INT} datatypes are slightly larger than \texttt{MPI\_CHAR} datatypes.
	
	I next changed both \texttt{MPI\_Send} and \texttt{MPI\_Recv} to have a datatype of \texttt{MPI\_INT} and \texttt{count} back to $100$. This worked without error or warning and produced similar output to above, just in a different order. To try and find a lower bound for what \texttt{count} can be with my message, I set \texttt{datatype} back to \texttt{MPI\_CHAR} for both functions since I am working with \texttt{char} C types. I then found that for single digit process numbers, the greeting message is $25$ characters long plus the \texttt{null} terminating character that C adds, making the message $26$ \texttt{char}s long. I changed \texttt{count} to $26$ in \texttt{MPI\_Recv} since the length of the message is calculated in \texttt{MPI\_Send}. This also ran and surprisingly the messages were printed in order. I ran the same code twice more and got the same results. This is intriguing because I kept \texttt{MPI\_ANY\_SOURCE} and \texttt{MPI\_ANY\_TAG} in the \texttt{MPI\_Recv} function. I could run more tests, but for these brief experiments these results are enough to tell me it may not be a coincidence and could have something to do with how MPI pipelines data from processes. To see what happens when I reduce \texttt{count} to just $25$ in \texttt{MPI\_Recv}. This resulted in an error telling me $26$ bytes were received but the buffer size is only $25$ and caused the program to hang. These results suggest that for future programs, either an upper limit should be placed on messages constructed by sending processes, some method of predicting message size should be implemented by the receiving processes, or \texttt{count} should be assigned to some large value where it is only assumed no message will be constructed to be larger. Of all three, I believe the best to be setting an upper limit on messages constructed by sending processes.
	
	I increased \texttt{count} back to $26$ so I could start manipulating the \texttt{source} and \texttt{dest} parameters. I started by hard coding \texttt{source} to always be $2$ in \texttt{MPI\_Recv}. This caused only the greeting from process $2$ to be printed and the program to hang. This is because the receive function is inside a for-loop trying to get a message from process $2$ four times, once for each process. However in this case each process sends only a single message meaning process $0$ will only receive $1$ message from process $2$. I used \texttt{scancel} to cancel my job because I knew it would never finish. I changed \texttt{source} back to \texttt{MPI\_ANY\_SOURCE} for further testing. Changing \texttt{dest} in \texttt{MPI\_Send} to any value other than $0$ caused the program to hang without printing anything out so I had to cancel the job to stop it. This happened because process $0$ is waiting for $4$ messages, but all $4$ processes that send messages are sending them to process $2$, which is not expecting them.
	
	\lstinputlisting[language=bash, caption=\texttt{run\_greetings\_1\_5.slurm} used to run code after each modification]{../run_greetings_1_5.slurm}
	

\pagebreak
\section{Problem 2}

\end{document}






























